{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Benchmarks to help with design/architecture decisions of the lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from sparse_autoencoder.autoencoder.model import SparseAutoencoder\n",
    "from sparse_autoencoder.dataset.dataloader import (\n",
    "    collate_neel_c4_tokenized,\n",
    "    create_dataloader,\n",
    ")\n",
    "from sparse_autoencoder.activations.ListActivationStore import ListActivationStore\n",
    "from sparse_autoencoder.train.train import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Tensor Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to know both the size and how much they can be compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of text data\n",
    "dataset = load_dataset(\"NeelNanda/c4-code-tokenized-2b\", split=\"train\", streaming=True)\n",
    "first_batch = []\n",
    "for idx, example in enumerate(dataset):\n",
    "    if not idx <= 24:\n",
    "        break\n",
    "    first_batch.append(example[\"tokens\"])\n",
    "first_batch = torch.tensor(first_batch)\n",
    "f\"Number of activations to store in this benchmark test: {first_batch.numel()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the activations\n",
    "src_model = HookedTransformer.from_pretrained(\"NeelNanda/GELU_1L512W_C4_Code\")\n",
    "logits, cache = src_model.run_with_cache(first_batch)\n",
    "activations = cache[\"blocks.0.mlp.hook_post\"].half()\n",
    "number_activations = activations.numel()\n",
    "size_bytes_activations = number_activations * 2  # Assume float 16\n",
    "size_mb_activations = f\"{size_bytes_activations / (10**6):.2f} MB\"\n",
    "f\"With {activations.numel()} features at half precision, the features take up {size_mb_activations} of memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try compressing on the disk (and find the impact is small so probably not worth it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to temp dir\n",
    "temp_dir = tempfile.gettempdir()\n",
    "temp_file = temp_dir + \"/temp.pt\"\n",
    "temp_file_gz = temp_file + \".gz\"\n",
    "torch.save(activations, temp_file)\n",
    "\n",
    "# Zip it\n",
    "with open(temp_file, \"rb\") as f_in:\n",
    "    with gzip.open(temp_file_gz, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# Get the file size back\n",
    "fs_bytes = os.path.getsize(temp_file_gz)\n",
    "f\"Compressed file size is {fs_bytes / (10**6):.2f} MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate assuming 8 billion activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumed_n_activation_batches = 8 * (10**9)\n",
    "assumed_n_activations_per_batch = 2048\n",
    "uncompressed_size_per_activation = 2  # float16\n",
    "estimated_size = (\n",
    "    assumed_n_activation_batches\n",
    "    * assumed_n_activations_per_batch\n",
    "    * uncompressed_size_per_activation\n",
    ")\n",
    "f\"With {assumed_n_activation_batches/10**9}B activations with {assumed_n_activations_per_batch} features, \\\n",
    "the estimated size is {estimated_size / (10**12):.2f} TB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the amount of activations you can store with different sizes\n",
    "sizes_gb = [10, 50, 100, 300, 500, 1000]\n",
    "activations_per_size = [\n",
    "    i * (10**9) / uncompressed_size_per_activation / assumed_n_activations_per_batch\n",
    "    for i in sizes_gb\n",
    "]\n",
    "\n",
    "table = pd.DataFrame({\"Size (GB)\": sizes_gb, \"Activations\": activations_per_size})\n",
    "table[\"Activations\"] = table[\"Activations\"].apply(\n",
    "    lambda x: \"{:,.0f}\".format(x / 10**6) + \"M\"\n",
    ")\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VastAI systems often have quite a lot of HD space (e.g. 300GB) but available ram is often smaller\n",
    "(e.g. 50GB and we need a reasonable amount left over for moving tensors around etc). This means that\n",
    "we can store c. 5-10M activations on a typical instance in CPU RAM (sometimes 25M+), or 50-100M on\n",
    "disk. Both seem like plenty!\n",
    "\n",
    "To note that replenishing a buffer of cached activations when half used in training seems like a lot\n",
    "of pain, considering that the improvement is likely marginal. Particularly if we also randomly sort\n",
    "the prompts for the forward pass of the source model, we'll have a chance of two tokens coming from\n",
    "the same/nearby prompts as very small.\n",
    "\n",
    "The conclusion is therefore that we do a need some sort of buffer, as we can't store 40TB on disk\n",
    "easily, and this buffer can be disk or ram. It needs to store asynchronously (so it doesn't block\n",
    "the forward pass), and it needs to be able to handle multiple simultaneous writes from e.g.\n",
    "distributed GPUs. The best approaches here are probably (a) pre-allocating a cpu ram space with\n",
    "torch.empty, or (b) writing asynchronously to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Activations (Forward Pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ListActivationStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark without multiprocessing:\n",
      "Storing 1000000 activations with 2056 features took 0.11 seconds.\n",
      "Equivalent time for 10B activations: 0.30 hours.\n",
      "\n",
      "Benchmark with multiprocessing:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ListActivationStore.__del__ at 0x2a8e40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alan/Documents/Repos/sparse_autoencoder/sparse_autoencoder/activations/ListActivationStore.py\", line 236, in __del__\n",
      "    self.finalise()\n",
      "  File \"/Users/alan/Documents/Repos/sparse_autoencoder/sparse_autoencoder/activations/ListActivationStore.py\", line 232, in finalise\n",
      "    self._thread_pool.shutdown(wait=True)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 235, in shutdown\n",
      "    t.join()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1116, in join\n",
      "    raise RuntimeError(\"cannot join current thread\")\n",
      "RuntimeError: cannot join current thread\n",
      "Exception ignored in: <function ListActivationStore.__del__ at 0x2a8e40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alan/Documents/Repos/sparse_autoencoder/sparse_autoencoder/activations/ListActivationStore.py\", line 236, in __del__\n",
      "    self.finalise()\n",
      "  File \"/Users/alan/Documents/Repos/sparse_autoencoder/sparse_autoencoder/activations/ListActivationStore.py\", line 232, in finalise\n",
      "    self._thread_pool.shutdown(wait=True)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 235, in shutdown\n",
      "    t.join()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1116, in join\n",
      "    raise RuntimeError(\"cannot join current thread\")\n",
      "RuntimeError: cannot join current thread\n",
      "Exception ignored in: <function ListActivationStore.__del__ at 0x2a8e40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alan/Documents/Repos/sparse_autoencoder/sparse_autoencoder/activations/ListActivationStore.py\", line 236, in __del__\n",
      "    self.finalise()\n",
      "  File \"/Users/alan/Documents/Repos/sparse_autoencoder/sparse_autoencoder/activations/ListActivationStore.py\", line 232, in finalise\n",
      "    self._thread_pool.shutdown(wait=True)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 235, in shutdown\n",
      "    t.join()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1116, in join\n",
      "    raise RuntimeError(\"cannot join current thread\")\n",
      "RuntimeError: cannot join current thread\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 1000000 activations with 2056 features took 0.03 seconds.\n",
      "Equivalent time for 10B activations: 0.10 hours.\n",
      "\n",
      "Benchmark with GPU & multiprocessing:\n",
      "Storing 1000000 activations with 2056 features took 0.11 seconds.\n",
      "Equivalent time for 10B activations: 0.32 hours.\n"
     ]
    }
   ],
   "source": [
    "# Benchmark storing activations\n",
    "def benchmark_list_activation_store(\n",
    "    n_items: int = 1_000_000,\n",
    "    n_features: int = 2056,\n",
    "    batch_size: int = 100,\n",
    "    model_device: torch.device = torch.device(\"cpu\"),\n",
    "    storage_device: torch.device = torch.device(\"cpu\"),\n",
    "    multiprocessing_enabled: bool = False,\n",
    "):\n",
    "    # Create the data\n",
    "    n_batches = int(n_items / batch_size)\n",
    "    data = [\n",
    "        torch.rand((batch_size, n_features), device=model_device)\n",
    "        for _ in range(n_batches)\n",
    "    ]\n",
    "\n",
    "    # Create the data store\n",
    "    dataset = ListActivationStore(\n",
    "        device=storage_device,\n",
    "        multiprocessing_enabled=multiprocessing_enabled,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    for batch in data:\n",
    "        dataset.extend(batch)\n",
    "\n",
    "    dataset.finalize()\n",
    "    print(len(dataset))\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(\n",
    "        f\"Storing {n_items} activations with {n_features} features took {duration:.2f} seconds.\"\n",
    "    )\n",
    "\n",
    "    equivalent_time_10b_activations = duration * (10**10) / n_items / (60 * 60)\n",
    "    print(\n",
    "        f\"Equivalent time for 10B activations: {equivalent_time_10b_activations:.2f} hours.\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Benchmark without multiprocessing:\")\n",
    "benchmark_list_activation_store()\n",
    "\n",
    "print(\"\\nBenchmark with multiprocessing:\")\n",
    "benchmark_list_activation_store(multiprocessing_enabled=True)\n",
    "\n",
    "print(\"\\nBenchmark with GPU & multiprocessing:\")\n",
    "benchmark_list_activation_store(\n",
    "    model_device=torch.device(\"mps\"),\n",
    "    storage_device=torch.device(\"mps\"),\n",
    "    multiprocessing_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare pre-creating a tensor in memory and then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_1(\n",
    "    batches_activations: list[Tensor],\n",
    "    n_items: int,\n",
    "    n_features: int,\n",
    "    batch_size: int,\n",
    "    storage_device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    \"\"\"Method 1: Using torch.empty to pre-allocate\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Setup storage\n",
    "        tensor_storage = torch.empty((n_items, n_features), device=storage_device)\n",
    "\n",
    "        # Append\n",
    "        start_time = time.time()\n",
    "        for idx, batch in enumerate(batches_activations):\n",
    "            start = idx * batch_size\n",
    "            end = start + batch_size\n",
    "            tensor_storage[start:end] = batch.to(storage_device)\n",
    "        end_time = time.time()\n",
    "        del tensor_storage\n",
    "\n",
    "        print(f\"Method 1 (torch.empty) time: {end_time - start_time:.5f} seconds\")\n",
    "\n",
    "\n",
    "def method_2(\n",
    "    batches_activations: list[Tensor],\n",
    "    storage_device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    \"\"\"Method 2: Appending to list\"\"\"\n",
    "    # Setup storage\n",
    "    result_list = []\n",
    "\n",
    "    # Append\n",
    "    start_time = time.time()\n",
    "    for idx, batch in enumerate(batches_activations):\n",
    "        for item in batch:\n",
    "            result_list.append(item.to(storage_device))\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Method 2 (appending to list) time: {end_time - start_time:.5f} seconds\")\n",
    "\n",
    "\n",
    "def run_test():\n",
    "    # Config\n",
    "    features = 512\n",
    "    batch_size = 10\n",
    "    batches = int(1_000_000 / batch_size)\n",
    "    items = int(batches * batch_size)\n",
    "\n",
    "    # Create the activations data\n",
    "    model_device = torch.device(\"cpu\")\n",
    "    activations = [\n",
    "        torch.randn((batch_size, features), device=model_device) for _ in range(batches)\n",
    "    ]\n",
    "\n",
    "    # Run the methods\n",
    "    print(\"CPU -> CPU:\")\n",
    "    method_1(activations, items, features, batch_size)\n",
    "    method_2(activations)\n",
    "\n",
    "    # Move to another device and test again\n",
    "    activations = [i.to(torch.device(\"mps\")) for i in activations]\n",
    "    print(\"MPS -> CPU:\")\n",
    "    method_1(activations, items, features, batch_size)\n",
    "    method_2(activations)\n",
    "\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly if they're already on the CPU, both methods are quite fast. But if they are both on\n",
    "the GPU, moving them across one at a time vs pre-allocating and then filling is very slow.\n",
    "\n",
    "This suggests pre-allocating is better from a write perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method1(\n",
    "    n_items: int,\n",
    "    n_features: int,\n",
    "    batch_size: int,\n",
    "    random_reads: int,\n",
    "    should_delete: bool = False,\n",
    ") -> None:\n",
    "    with torch.no_grad():\n",
    "        data = torch.randn((n_items, n_features))\n",
    "        has_data = torch.randn((n_items)) > 0.5\n",
    "\n",
    "        start_time = time.time()\n",
    "        batches = []\n",
    "        for i in range(random_reads):\n",
    "            has_data_indices = torch.where(has_data)[0]\n",
    "            random_indices = torch.randint(0, len(has_data_indices), (batch_size,))\n",
    "            items = data[has_data_indices[random_indices]]\n",
    "            batches.append(items)\n",
    "\n",
    "            if should_delete:\n",
    "                has_data[has_data_indices[random_indices]] = False\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"Method 1 (torch.empty) random read time: {end_time - start_time:.5f} seconds\"\n",
    "        )\n",
    "\n",
    "\n",
    "def method2(\n",
    "    n_items: int,\n",
    "    n_features: int,\n",
    "    batch_size: int,\n",
    "    random_reads: int,\n",
    "    should_delete: bool = False,\n",
    ") -> None:\n",
    "    data = [torch.randn((n_features,)) for _ in range(n_items)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    batches = []\n",
    "    for i in range(random_reads):\n",
    "        if should_delete:\n",
    "            data_np = np.array(data, dtype=object)\n",
    "            sampled_indices = np.random.choice(\n",
    "                data_np.shape[0], size=batch_size, replace=False\n",
    "            )\n",
    "            items = data_np[sampled_indices]\n",
    "            data = np.delete(data, sampled_indices).tolist()\n",
    "\n",
    "        else:\n",
    "            items = random.sample(data, batch_size)\n",
    "            batches.append(items)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Method 2 (list) random read time: {end_time - start_time:.5f} seconds\")\n",
    "\n",
    "\n",
    "def run():\n",
    "    n_items = 1_000_000\n",
    "    n_features = 512\n",
    "    batch_size = 10\n",
    "    random_reads = 1000\n",
    "\n",
    "    print(\"Without deleting:\")\n",
    "    method1(n_items, n_features, batch_size, random_reads)\n",
    "    method2(n_items, n_features, batch_size, random_reads)\n",
    "\n",
    "    print(\"With deleting:\")\n",
    "    method1(n_items, n_features, batch_size, random_reads, should_delete=True)\n",
    "    # method2(n_items, n_features, batch_size, random_reads, should_delete=True)\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random reads are substantially faster with list (torch.empty is slow), but we're still talking\n",
    "8h/TB (roughly) so this is still too slow.\n",
    "\n",
    "It seems like we need a better setup for reading e.g. `TensorDataset` or just a DataSet reading the\n",
    "list. Let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_dataset_test(\n",
    "    n_items: int,\n",
    "    n_features: int,\n",
    "    batch_size: int,\n",
    "    n_reads: int,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        data = torch.randn((n_items, n_features))\n",
    "        tensor_dataset = torch.utils.data.TensorDataset(data)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            tensor_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i in range(n_reads):\n",
    "            batch = next(iter(dataloader))\n",
    "        end_time = time.time()\n",
    "        print(\n",
    "            f\"With TensorDataset, reading {n_reads} took {end_time - start_time:.5f} seconds\"\n",
    "        )\n",
    "\n",
    "\n",
    "tensor_dataset_test(1_000_000, 512, 10, 1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is v. slow (dataloader shuffle has a big overhead compared to our optimum approach. Therefore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
